from typing import List, Dict, Any, Optional
from config.settings import global_settings
from datetime import datetime
import asyncio
import json

# Agno imports
from agno.agent import Agent
from agno.models.dashscope import DashScope
from agno.db.postgres import AsyncPostgresDb
from playwright.async_api import async_playwright

# å¯¼å…¥å¤–éƒ¨ Service
from .connectors.connector_service import ConnectorService
from models.task import Task
from utils.logger import logger

# 1. æ•°æ®åº“è¿æ¥
db = AsyncPostgresDb(
    db_url=f"postgresql+asyncpg://{global_settings.database.user}:{global_settings.database.password}@{global_settings.database.host}:{global_settings.database.port}/{global_settings.database.name}")

# 2. æ¨¡å‹é…ç½®
reasoning_model = DashScope(
    base_url=global_settings.external_service.aliyun_base_url,
    api_key=global_settings.external_service.aliyun_api_key,
    id="qwen-max-latest",
)

chat_model = DashScope(
    base_url=global_settings.external_service.aliyun_base_url,
    api_key=global_settings.external_service.aliyun_api_key,
    id="qwen-plus",
)

class XiaohongshuTrendAgent:
    """å°çº¢ä¹¦æ·±åº¦çˆ†æ¬¾åˆ†æä¸“å®¶"""

    def __init__(
            self,
            source_id: str = "system_user",
            source: str = "system",
            playwright: Any = None,
            keywords: List[str] = None,
            task: Task = None
    ):
        self._playwright = playwright
        self._task = task
        self._source = source
        self._source_id = source_id
        self.keywords = keywords
        self.current_date = datetime.now().strftime("%Y-%m-%d")

        # === æ ¸å¿ƒå˜åŒ– 1ï¼šAgent ä¸å†æŒ‚è½½ tools ===
        # å®ƒç°åœ¨åªæ˜¯ä¸€ä¸ªçº¯ç²¹çš„åˆ†æå¤§è„‘
        self.agent = Agent(
            name="çˆ†æ¬¾æ¢é’ˆ",
            model=chat_model,
            instructions=[
                f"å½“å‰æ—¥æœŸ: {self.current_date}ã€‚",
                "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æŒ–æ˜çˆ†æ¬¾é€»è¾‘çš„ä¸“å®¶ã€‚",
                "ç”¨æˆ·å·²ç»ä¸ºä½ å‡†å¤‡å¥½äº†ã€æœç´¢ç»“æœã€‘å’Œã€ç¬”è®°è¯¦æƒ…ã€‘çš„æ•°æ®ã€‚",
                "è¯·ä½ ç›´æ¥é˜…è¯»è¿™äº›æ•°æ®ï¼Œå®Œæˆä»¥ä¸‹åˆ†æï¼š",
                "1. **æ·±åº¦è§£ç **ï¼šåˆ†æç¬”è®°æ ‡é¢˜å¦‚ä½•åˆ¶é€ ç„¦è™‘/æœŸå¾…ï¼Ÿé¦–å›¾æœ‰ä½•è§†è§‰å¸ç›ç‚¹ï¼Ÿè¯„è®ºåŒºç—›ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "2. **è¾“å‡ºçˆ†æ¬¾çš„è¯¦ç»†ä¿¡æ¯**ï¼šåŸºäºæ•°æ®ï¼Œç»™å‡ºåŸæ–‡æ•°æ®ä¸çˆ†æ¬¾åˆ†æã€‚",
                "3. **è¾“å‡ºè¡ŒåŠ¨æŒ‡å—**ï¼šåŸºäºæ•°æ®ï¼Œç”Ÿæˆ 3 ä¸ªå…·ä½“çš„çˆ†æ¬¾é€‰é¢˜æ–¹æ¡ˆå’Œå»ºè®®ã€‚",
                "4. **è¯æ®é“¾æ¡**ï¼šé‡è¦ï¼åœ¨åˆ†ææ¯ä¸ªè§‚ç‚¹æ—¶ï¼Œå¿…é¡»å¼•ç”¨å…·ä½“ç¬”è®°çš„å®Œæ•´URLï¼ˆå­—æ®µfull_urlï¼‰ä½œä¸ºè¯æ®ï¼Œè®©åˆ†æå¯è¿½æº¯ã€‚"
            ],
            db=db,
            markdown=True,
            add_history_to_context=True,
            user_id=source_id,
        )

        # ç”¨äºç”Ÿæˆå…³é”®è¯çš„å°å· Agent (è½»é‡çº§)
        self.planner = Agent(model=reasoning_model, description="å…³é”®è¯è£‚å˜åŠ©æ‰‹")

    # === æ ¸å¿ƒå˜åŒ– 2ï¼šå·¥å…·å˜æˆäº†æ™®é€šçš„ Python å¼‚æ­¥æ–¹æ³• ===
    # è¿™äº›æ–¹æ³•ä¸å†è¢« Agent è‡ªåŠ¨è°ƒç”¨ï¼Œè€Œæ˜¯è¢« Python é€»è¾‘æ˜¾å¼è°ƒç”¨

    async def _generate_keywords(self) -> List[str]:
        """å‰ç½®å·¥ä½œ Step 1: è£‚å˜å…³é”®è¯"""
        logger.info("æ­£åœ¨è£‚å˜å…³é”®è¯...")
        prompt = f"è¯·åŸºäºæ ¸å¿ƒè¯ã€Œ{self.keywords}ã€èåˆè¿™ä¸‰ä¸ªç‚¹ï¼Œè£‚å˜å‡º 3 ä¸ªä¸åŒç»´åº¦çš„æœç´¢è¯ï¼ˆæ ¸å¿ƒè¯ã€åœºæ™¯è¯ã€ç—›ç‚¹è¯ï¼‰ã€‚åªè¿”å›é€—å·åˆ†éš”çš„å…³é”®è¯å­—ç¬¦ä¸²ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
        resp = await self.planner.arun(prompt)
        # ç®€å•çš„æ¸…æ´—é€»è¾‘
        keywords = [k.strip() for k in resp.content.replace("ï¼Œ", ",").split(",") if k.strip()]
        return keywords  # ç¡®ä¿åªå–å‰3ä¸ª

    async def _run_search(self, keywords: List[str], task: Task, connector_service, limit: int = 10) -> List[Dict]:
        """å‰ç½®å·¥ä½œ Step 2: æ‰§è¡Œæœç´¢"""
        from models.connectors import PlatformType
        logger.info(f"æ­£åœ¨æ‰§è¡Œæœç´¢: {keywords}")

        # è®°å½•å¼€å§‹æœç´¢
        await task.log_step(
            2,
            "æ‰§è¡Œæœç´¢",
            {
                "keywords": keywords,
                "limit": limit
            },
            {
                "status": f"å¼€å§‹æœç´¢ {len(keywords)} ä¸ªå…³é”®è¯"
            }
        )

        raw_results = await connector_service.search_and_extract(
            platform=PlatformType.XIAOHONGSHU,
            keywords=keywords,
            limit=limit
        )

        all_notes = []
        for res in raw_results:
            if res.get("success"):
                all_notes.extend(res.get("data", []))

        # === å»é‡é€»è¾‘ï¼šåŸºäºå¸–å­å”¯ä¸€æ ‡è¯† ===
        # ä½¿ç”¨ note_id æˆ– full_url ä½œä¸ºå”¯ä¸€æ ‡è¯†
        seen_note_ids = set()
        unique_notes = []

        for note in all_notes:
            # ä¼˜å…ˆä½¿ç”¨ note_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ full_url
            note_id = note.get("note_id") or note.get("full_url")

            if note_id and note_id not in seen_note_ids:
                seen_note_ids.add(note_id)
                unique_notes.append(note)

        logger.info(f"æœç´¢ç»“æœå»é‡: {len(all_notes)} æ¡ -> {len(unique_notes)} æ¡å”¯ä¸€å¸–å­")

        # æŒ‰ç‚¹èµæ•°å€’åºï¼Œå–å‰ 10 ä¸ªæœ€æœ‰ä»·å€¼çš„
        sorted_notes = sorted(unique_notes, key=lambda x: x.get("liked_count", 0), reverse=True)
        top_notes = sorted_notes[:10]

        # è®°å½•æœç´¢å®Œæˆ
        await task.log_step(
            2,
            "æ‰§è¡Œæœç´¢",
            {
                "keywords": keywords
            },
            {
                "status": f"æœç´¢å®Œæˆï¼Œè·å¾— {len(all_notes)} æ¡ç»“æœï¼Œå»é‡å {len(unique_notes)} æ¡",
                "raw_count": len(all_notes),
                "unique_count": len(unique_notes),
                "top_count": len(top_notes)
            }
        )

        return top_notes

    async def _fetch_details(self, notes: List[Dict], task: Task, connector_service) -> str:
        """å‰ç½®å·¥ä½œ Step 3: æŠ“å–è¯¦æƒ…å¹¶æ‹¼æ¥æˆæ–‡æœ¬"""
        from models.connectors import PlatformType

        # æå– URL
        urls = [n.get("full_url") for n in notes if n.get("full_url")]
        logger.info(f"æ­£åœ¨æŠ“å–è¯¦æƒ…ï¼Œå…± {len(urls)} ç¯‡")

        # è®°å½•å¼€å§‹è·å–è¯¦æƒ…
        await task.log_step(
            3,
            "è·å–ç¬”è®°è¯¦æƒ…",
            {
                "note_count": len(notes),
                "urls": urls[:3]  # åªè®°å½•å‰3ä¸ªURLä½œä¸ºç¤ºä¾‹
            },
            {
                "status": f"å¼€å§‹è·å– {len(urls)} ç¯‡ç¬”è®°çš„è¯¦æƒ…"
            }
        )

        # åˆ†æ‰¹è·å–ç¬”è®°è¯¦æƒ…ï¼Œé¿å…æµè§ˆå™¨æ‹¨æ‰“è¿‡å¿«
        batch_size = 3
        all_details_results = []

        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i + batch_size]
            logger.info(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}, URLs: {batch_urls}")

            batch_results = await connector_service.get_note_details(
                urls=batch_urls,
                platform=PlatformType.XIAOHONGSHU,
                concurrency=2  # æ¯æ‰¹å†…éƒ¨å¹¶å‘2ä¸ª
            )

            all_details_results.extend(batch_results)
            logger.info(f"æ‰¹æ¬¡ {i//batch_size + 1} å®Œæˆï¼Œè·å– {len(batch_results)} ä¸ªç»“æœ")

        details_results = all_details_results

        # æ„å»º url -> detail æ˜ å°„
        details_map = {}
        success_count = 0
        for result in details_results:
            if result.get("success") and result.get("data"):
                details_map[result.get("url")] = result.get("data", {})
                success_count += 1

        logger.info(f"è¯¦æƒ…è·å–å®Œæˆ: {success_count}/{len(urls)} æˆåŠŸ")

        # è®°å½•æ¯ç¯‡ç¬”è®°çš„è¯¦æƒ…
        for i, note in enumerate(notes):
            url = note.get("full_url")
            detail = details_map.get(url, {})

            title = detail.get("title") or note.get("title", "æœªçŸ¥æ ‡é¢˜")
            liked_count = detail.get("liked_count", note.get("liked_count", 0))

            if detail and title:
                # æˆåŠŸè·å–è¯¦æƒ…
                await task.log_step(
                    3,
                    f"è§£æç¬”è®° [{i+1}/{len(notes)}]",
                    {
                        "note_index": i + 1,
                        "title": title,
                        "url": url
                    },
                    {
                        "status": f"è§£ææˆåŠŸ: {title[:30]}",
                        "liked_count": liked_count,
                        "has_detail": True
                    }
                )
            else:
                # è·å–è¯¦æƒ…å¤±è´¥
                await task.log_step(
                    3,
                    f"è§£æç¬”è®° [{i+1}/{len(notes)}]",
                    {
                        "note_index": i + 1,
                        "url": url
                    },
                    {
                        "status": "è§£æå¤±è´¥",
                        "has_detail": False
                    }
                )

        # æ‹¼æ¥ context æ–‡æœ¬
        context_parts = []
        for i, note in enumerate(notes):
            url = note.get("full_url")
            detail = details_map.get(url, {})

            # æå–è¯¦æƒ…æ•°æ®ï¼ˆä½¿ç”¨æ–°çš„æ‰å¹³åŒ–å­—æ®µï¼‰
            title = detail.get("title") or note.get("title", "æœªçŸ¥æ ‡é¢˜")
            desc = detail.get("desc", "")

            # äº’åŠ¨æ•°æ®å·²ç»æ˜¯æ‰å¹³åŒ–çš„æ•´æ•°
            liked_count = detail.get("liked_count", note.get("liked_count", 0))
            collected_count = detail.get("collected_count", 0)
            comment_count = detail.get("comment_count", 0)

            # å›¾ç‰‡å’Œè¯„è®º
            images = detail.get("images", [])
            cover_url = images[0].get("url") if images else None
            comments = detail.get("comments", [])

            # æ ¼å¼åŒ–è¯„è®ºï¼ˆå‰3æ¡ï¼‰
            comment_str = ""
            if comments:
                top_comments = comments[:3]
                comment_texts = [
                    f"- {c.get('content', '')[:50]}..."
                    for c in top_comments if c.get("content")
                ]
                comment_str = "\n".join(comment_texts)
            else:
                comment_str = "æš‚æ— è¯„è®º"

            note_str = (
                f"ã€ç¬”è®° {i + 1}ã€‘\n"
                f"æ ‡é¢˜: {title}\n"
                f"å°é¢: {cover_url}\n"
                f"é“¾æ¥: {url}\n"
                f"äº’åŠ¨æ•°æ®: ç‚¹èµ{liked_count} | æ”¶è—{collected_count} | è¯„è®º{comment_count}\n"
                f"æ­£æ–‡å†…å®¹:\n{desc}\n\n"
                f"ç²¾é€‰è¯„è®º:\n{comment_str}\n"
                f"{'='*60}"
            )
            context_parts.append(note_str)

        return "\n\n".join(context_parts)

    async def execute(self, task: Task) -> str:
        """
        æ‰§è¡Œè¶‹åŠ¿åˆ†æä»»åŠ¡ - ç»Ÿä¸€å…¥å£æ–¹æ³•

        Args:
            task: å·²åˆ›å»ºçš„ä»»åŠ¡å¯¹è±¡

        Returns:
            åˆ†æç»“æœ
        """
        # è®¾ç½® taskï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ ConnectorService
        self._task = task

        try:
            if not self.keywords:
                # è®°å½•é”™è¯¯å‚æ•°
                await task.fail("æ— è¾“å…¥ï¼Œè¯·è¾“å…¥æœ‰æ•ˆå…³é”®å­—é‡è¯•", 0)
                await task.save()
                return "æ— è¾“å…¥ï¼Œè¯·è¾“å…¥æœ‰æ•ˆå…³é”®å­—é‡è¯•"

            # è®°å½•åˆå§‹å‚æ•°
            await task.log_step(0, "ä»»åŠ¡åˆå§‹åŒ–",
                              {"keywords": self.keywords},
                              {"task_id": str(task.id), "source": self._source})
            task.progress = 10
            await task.save()

            # === AI Native ç™»å½•æ£€æŸ¥ ===
            # åœ¨æ‰§è¡Œä»»åŠ¡å‰ï¼Œå…ˆæ£€æŸ¥å¹³å°ç™»å½•çŠ¶æ€
            from services.sniper.connectors.xiaohongshu import XiaohongshuConnector

            connector = XiaohongshuConnector(playwright=self._playwright)

            # è°ƒç”¨å…¬å…±æ–¹æ³•æ£€æŸ¥ç™»å½•çŠ¶æ€
            # æ–¹æ³•å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç† sessionã€browserã€context çš„åˆ›å»ºå’Œæ¸…ç†
            is_logged_in, resource_url = await connector.check_login_status(
                source=self._source,
                source_id=self._source_id
            )

            if not is_logged_in:
                # æœªç™»å½•ï¼Œæš‚åœä»»åŠ¡å¹¶ä¿å­˜ç™»å½•ä¿¡æ¯
                await task.waiting_login({
                    "platform": "xiaohongshu",
                    "context_id": "",  # ç™»å½•æ—¶ä¼šè·å–
                    "resource_url": resource_url
                })
                logger.info(f"[xhs_trend] ä»»åŠ¡ {task.id} ç­‰å¾…ç™»å½•")
                return "ç­‰å¾…ç™»å½•"

            # Step 1: å…³é”®è¯è£‚å˜
            search_keywords = await self._generate_keywords()
            await task.log_step(1, "å…³é”®è¯è£‚å˜",
                              {"core_keyword": self.keywords},
                              {"keywords": search_keywords})
            task.progress = 25
            await task.save()

            # ä½¿ç”¨ async with ConnectorService
            async with ConnectorService(self._playwright, self._source, self._source_id, self._task) as connector_service:
                # Step 2: æœç´¢å¹¶å»é‡
                top_notes = await self._run_search(search_keywords, task, connector_service)
                if not top_notes:
                    await task.fail("æœªæœç´¢åˆ°æœ‰æ•ˆæ•°æ®", task.progress)
                    return ""

                task.progress = 50
                await task.save()

                # Step 3: è·å–è¯¦æƒ…
                context_data = await self._fetch_details(top_notes, task, connector_service)

                # è®°å½•è·å–è¯¦æƒ…å®Œæˆ
                await task.log_step(3, "è·å–ç¬”è®°è¯¦æƒ…",
                                  {"note_count": len(top_notes)},
                                  {
                                    "status": f"è¯¦æƒ…è·å–å®Œæˆï¼Œå…± {len(context_data)} å­—ç¬¦",
                                    "context_length": len(context_data)
                                  })
                task.progress = 70
                await task.save()

                # Step 4: Agent åˆ†æ
                prompt = f"""
                ä»»åŠ¡æ ¸å¿ƒè¯ï¼š{self.keywords}

                ä»¥ä¸‹æ˜¯æˆ‘ä¸ºä½ é‡‡é›†åˆ°çš„æœ€æ–°æ•°æ®ï¼š
                {context_data}

                è¯·æ ¹æ® instructions å¼€å§‹åˆ†æã€‚

                **é‡è¦æé†’**ï¼šåœ¨è¾“å‡ºåˆ†æå’Œå»ºè®®æ—¶ï¼Œå¿…é¡»ä¸ºæ¯ä¸ªè§‚ç‚¹æä¾›è¯æ®é“¾æ¡ï¼Œå¼•ç”¨å…·ä½“ç¬”è®°çš„å®Œæ•´URLï¼ˆfull_urlï¼‰ã€‚
                """

                analysis_result = await self.agent.arun(prompt)
                analysis = analysis_result.content

                await task.log_step(4, "Agentåˆ†æ",
                                  {"data_size": len(context_data)},
                                  {"analysis_length": len(analysis)})
                task.progress = 95
                await task.save()

                # AI Native: Agent çš„åˆ†æç»“æœæœ¬èº«å°±æ˜¯è‡ªç„¶è¯­è¨€ï¼Œç›´æ¥å­˜å‚¨
                # æ— éœ€é¢å¤–æ ¼å¼åŒ–ï¼ŒLLM ç”Ÿæˆçš„åˆ†æç»“æœå°±æ˜¯æœ€é€‚åˆ AI é˜…è¯»çš„æ ¼å¼
                await task.complete({"analysis": analysis})
                return analysis

        except Exception as e:
            logger.error(f"è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            await task.fail(str(e), task.progress)
            raise

    async def analyze_trends_stream(self):
        """
        æµå¼ä»»åŠ¡å…¥å£ - ç¼–æ’é€»è¾‘
        """
        yield "ğŸš€ [Step 1] æ­£åœ¨è¿›è¡Œå…³é”®è¯è£‚å˜...\n"
        search_keywords = await self._generate_keywords()
        yield f" -> è£‚å˜ç»“æœ: {search_keywords}\n"

        # ä½¿ç”¨ async with ConnectorService
        async with ConnectorService(self._playwright, self._source, self._source_id, self._task) as connector_service:
            yield "ğŸ” [Step 2] æ­£åœ¨å¤šçº¿ç¨‹å¹¶å‘æœç´¢...\n"
            top_notes = await self._run_search_no_task(search_keywords, connector_service)
            yield f" -> ç­›é€‰å‡º {len(top_notes)} ç¯‡å¤´éƒ¨ç¬”è®°\n"

            if not top_notes:
                yield "âŒ æœªæœç´¢åˆ°æœ‰æ•ˆæ•°æ®ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚"
                return

            yield "ğŸ“– [Step 3] æ­£åœ¨é˜…è¯»ç¬”è®°è¯¦æƒ…...\n"
            context_data = await self._fetch_details_no_task(top_notes, connector_service)

            yield "ğŸ§  [Step 4] æ•°æ®å‡†å¤‡å®Œæ¯•ï¼ŒAgent å¼€å§‹æ·±åº¦åˆ†æ...\n\n"

            # === æ ¸å¿ƒï¼šæŠŠå‡†å¤‡å¥½çš„æ•°æ®å–‚ç»™ Agent ===
            prompt = f"""
            ä»»åŠ¡æ ¸å¿ƒè¯ï¼š{self.keywords}

            ä»¥ä¸‹æ˜¯æˆ‘ä¸ºä½ é‡‡é›†åˆ°çš„æœ€æ–°æ•°æ®ï¼š
            {context_data}

            è¯·æ ¹æ® instructions å¼€å§‹åˆ†æã€‚

            **é‡è¦æé†’**ï¼šåœ¨è¾“å‡ºåˆ†æå’Œå»ºè®®æ—¶ï¼Œå¿…é¡»ä¸ºæ¯ä¸ªè§‚ç‚¹æä¾›è¯æ®é“¾æ¡ï¼Œå¼•ç”¨å…·ä½“ç¬”è®°çš„å®Œæ•´URLï¼ˆfull_urlï¼‰ã€‚
            """

            async for chunk in self.agent.arun(prompt, stream=True):
                if chunk and chunk.content:
                    yield chunk.content

    async def _run_search_no_task(self, keywords: List[str], connector_service, limit: int = 10) -> List[Dict]:
        """å‰ç½®å·¥ä½œ Step 2: æ‰§è¡Œæœç´¢ (ä¸å¸¦ taskï¼Œç”¨äº stream ç‰ˆæœ¬)"""
        from models.connectors import PlatformType
        logger.info(f"æ­£åœ¨æ‰§è¡Œæœç´¢: {keywords}")

        raw_results = await connector_service.search_and_extract(
            platform=PlatformType.XIAOHONGSHU,
            keywords=keywords,
            limit=limit,
            concurrency=2
        )

        all_notes = []
        for res in raw_results:
            if res.get("success"):
                all_notes.extend(res.get("data", []))

        # === å»é‡é€»è¾‘ï¼šåŸºäºå¸–å­å”¯ä¸€æ ‡è¯† ===
        seen_note_ids = set()
        unique_notes = []

        for note in all_notes:
            note_id = note.get("note_id") or note.get("full_url")
            if note_id and note_id not in seen_note_ids:
                seen_note_ids.add(note_id)
                unique_notes.append(note)

        logger.info(f"æœç´¢ç»“æœå»é‡: {len(all_notes)} æ¡ -> {len(unique_notes)} æ¡å”¯ä¸€å¸–å­")

        sorted_notes = sorted(unique_notes, key=lambda x: x.get("liked_count", 0), reverse=True)
        return sorted_notes[:10]

    async def _fetch_details_no_task(self, notes: List[Dict], connector_service) -> str:
        """å‰ç½®å·¥ä½œ Step 3: æŠ“å–è¯¦æƒ…å¹¶æ‹¼æ¥æˆæ–‡æœ¬ (ä¸å¸¦ taskï¼Œç”¨äº stream ç‰ˆæœ¬)"""
        from models.connectors import PlatformType

        urls = [n.get("full_url") for n in notes if n.get("full_url")]
        logger.info(f"æ­£åœ¨æŠ“å–è¯¦æƒ…ï¼Œå…± {len(urls)} ç¯‡")

        # åˆ†æ‰¹è·å–ç¬”è®°è¯¦æƒ…ï¼Œé¿å…æµè§ˆå™¨æ‹¨æ‰“è¿‡å¿«
        batch_size = 3
        all_details_results = []

        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i + batch_size]
            logger.info(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}, URLs: {batch_urls}")

            batch_results = await connector_service.get_note_details(
                urls=batch_urls,
                platform=PlatformType.XIAOHONGSHU,
                concurrency=2  # æ¯æ‰¹å†…éƒ¨å¹¶å‘2ä¸ª
            )

            all_details_results.extend(batch_results)
            logger.info(f"æ‰¹æ¬¡ {i//batch_size + 1} å®Œæˆï¼Œè·å– {len(batch_results)} ä¸ªç»“æœ")

        details_results = all_details_results

        # æ„å»º url -> detail æ˜ å°„
        details_map = {}
        for result in details_results:
            if result.get("success") and result.get("data"):
                details_map[result.get("url")] = result.get("data", {})

        # æ‹¼æ¥ context æ–‡æœ¬
        context_parts = []
        for i, note in enumerate(notes):
            url = note.get("full_url")
            detail = details_map.get(url, {})

            title = detail.get("title") or note.get("title", "æœªçŸ¥æ ‡é¢˜")
            desc = detail.get("desc", "")

            liked_count = detail.get("liked_count", note.get("liked_count", 0))
            collected_count = detail.get("collected_count", 0)
            comment_count = detail.get("comment_count", 0)

            images = detail.get("images", [])
            cover_url = images[0].get("url") if images else None
            comments = detail.get("comments", [])

            # æ ¼å¼åŒ–è¯„è®ºï¼ˆå‰3æ¡ï¼‰
            comment_str = ""
            if comments:
                top_comments = comments[:3]
                comment_texts = [
                    f"- {c.get('content', '')[:50]}..."
                    for c in top_comments if c.get("content")
                ]
                comment_str = "\n".join(comment_texts)
            else:
                comment_str = "æš‚æ— è¯„è®º"

            note_str = (
                f"ã€ç¬”è®° {i + 1}ã€‘\n"
                f"æ ‡é¢˜: {title}\n"
                f"å°é¢: {cover_url}\n"
                f"é“¾æ¥: {url}\n"
                f"äº’åŠ¨æ•°æ®: ç‚¹èµ{liked_count} | æ”¶è—{collected_count} | è¯„è®º{comment_count}\n"
                f"æ­£æ–‡å†…å®¹:\n{desc}\n\n"
                f"ç²¾é€‰è¯„è®º:\n{comment_str}\n"
                f"{'='*60}"
            )
            context_parts.append(note_str)

        return "\n\n".join(context_parts)


# ========== è„šæœ¬ä¸»ç¨‹åº ==========
async def main():
    source = "service"
    source_id = "default"
    from tortoise import Tortoise
    from config.settings import create_db_config

    await Tortoise.init(config=create_db_config())

    start_time = datetime.now()
    print("=== å°çº¢ä¹¦trendåˆ†æå¯åŠ¨ ===", flush=True)
    print(f"â° ä»»åŠ¡å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)

    async with async_playwright() as p:
        try:
            # åˆ›å»ºä»»åŠ¡
            task = await Task.create(
                source=source,
                source_id=source_id,
                task_type="trend_analysis"
            )
            await task.start()

            keywords = ["SKG", "å¥åº·ç©¿æˆ´", "æŒ‰æ‘©ä»ª"]
            keywords = ["åç«¯å¼€å‘", "Agent"]
            # é‡æ–°åˆ›å»º analyzerï¼Œä¼ å…¥ task
            analyzer = XiaohongshuTrendAgent(
                source_id=source_id,
                source=source,
                playwright=p,
                keywords=keywords,
                task=task
            )

            print(f"[æ ¸å¿ƒè¯]: {analyzer.keywords}")
            print("-" * 30)
            print(f"[Task ID]: {task.id}")

            # æ‰§è¡Œåˆ†æ
            analysis = await analyzer.execute(task=task)
            print(analysis, flush=True)

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            print("\n" + "-" * 80)
            print("[ä»»åŠ¡ç»“æŸ]")
            print(f"â° ä»»åŠ¡ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
            print(f"â±ï¸  ä»»åŠ¡è€—æ—¶: {duration:.2f} ç§’", flush=True)

        except Exception as e:
            print(f"\nè¿è¡Œæ—¶å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())